# -*- coding: utf-8 -*-
"""r-cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16vzDWK3NSKaoBo6Bo-R4icKsICwVW3Nr
"""

from google.colab import drive
drive.mount('/content/drive')

# ‚úÖ Install dependencies
!pip install torch torchvision matplotlib

# ‚úÖ Imports
import os
import torch
from PIL import Image
import matplotlib.pyplot as plt
from torchvision import transforms
from torch.utils.data import DataLoader
from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.ops import box_iou

# ‚úÖ Dataset class with class fix and box validation
class YoloDataset(torch.utils.data.Dataset):
    def __init__(self, images_dir, labels_dir, transforms=None):
        self.images_dir = images_dir
        self.labels_dir = labels_dir
        self.transforms = transforms
        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))])

    def __getitem__(self, idx):
        img_path = os.path.join(self.images_dir, self.image_files[idx])
        label_path = os.path.join(self.labels_dir, self.image_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))

        img = Image.open(img_path).convert("RGB")
        W, H = img.size

        boxes, labels = [], []
        with open(label_path) as f:
            for line in f:
                cls, x_center, y_center, w, h = map(float, line.strip().split())
                x1 = (x_center - w/2) * W
                y1 = (y_center - h/2) * H
                x2 = (x_center + w/2) * W
                y2 = (y_center + h/2) * H
                if x2 > x1 and y2 > y1:
                    boxes.append([x1, y1, x2, y2])
                    labels.append(1)  # Force class to 1 (foreground)

        boxes = torch.as_tensor(boxes if boxes else [[0,0,1,1]], dtype=torch.float32)
        labels = torch.as_tensor(labels if labels else [1], dtype=torch.int64)

        target = {"boxes": boxes, "labels": labels}
        if self.transforms:
            img = self.transforms(img)
        return img, target

    def __len__(self):
        return len(self.image_files)

# ‚úÖ Model setup
def get_model(num_classes):
    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
    model = fasterrcnn_resnet50_fpn(weights=weights)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model

# ‚úÖ Accuracy evaluation
def evaluate_accuracy(model, data_loader, device, iou_threshold=0.3):
    model.eval()
    TP, FP, FN = 0, 0, 0

    with torch.no_grad():
        for images, targets in data_loader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            outputs = model(images)

            for pred, target in zip(outputs, targets):
                pred_boxes = pred["boxes"]
                true_boxes = target["boxes"]

                if len(true_boxes) == 0:
                    FP += len(pred_boxes)
                    continue

                ious = box_iou(pred_boxes, true_boxes)
                matched_pred = (ious.max(dim=1).values >= iou_threshold)
                matched_true = (ious.max(dim=0).values >= iou_threshold)

                TP += matched_pred.sum().item()
                FP += (~matched_pred).sum().item()
                FN += (~matched_true).sum().item()

    total = TP + FP + FN
    accuracy = TP / total if total > 0 else 0
    return accuracy

# ‚úÖ Inference and visualization
def predict_and_show(image_path, model, device, threshold=0.3):
    print(f"\nüîç Predicting: {image_path}")
    if not os.path.exists(image_path):
        print("‚ùå Image not found.")
        return
    model.eval()
    img = Image.open(image_path).convert("RGB")
    img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(img_tensor)
    boxes = outputs[0]['boxes'].cpu().numpy()
    scores = outputs[0]['scores'].cpu().numpy()
    if len(boxes) == 0 or all(score < threshold for score in scores):
        print("‚ö†Ô∏è No objects detected above threshold.")
    else:
        for i, (box, score) in enumerate(zip(boxes, scores)):
            if score >= threshold:
                print(f"Box {i+1}: {box} | Confidence: {score:.2f}")
    plt.imshow(img)
    ax = plt.gca()
    for box, score in zip(boxes, scores):
        if score >= threshold:
            x1, y1, x2, y2 = box
            ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, edgecolor='red', facecolor='none'))
            ax.text(x1, y1, f"{score:.2f}", color='white', backgroundcolor='red')
    plt.axis('off')
    plt.show()

# ‚úÖ Paths (update if needed)
base_path = '/content/drive/MyDrive/DL_Datasets/Glass-Breakage-1'
train_images = f'{base_path}/train/images'
train_labels = f'{base_path}/train/labels'
valid_images = f'{base_path}/valid/images'
valid_labels = f'{base_path}/valid/labels'
test_image = f'{base_path}/valid/images/DJI_0450_R_JPG.rf.a9b53c1a528b6e1c2582f7a4f6d3a07e.jpg'  # Replace with actual filename

# ‚úÖ Load data
transform = transforms.ToTensor()
train_dataset = YoloDataset(train_images, train_labels, transforms=transform)
valid_dataset = YoloDataset(valid_images, valid_labels, transforms=transform)
collate_fn = lambda x: tuple(zip(*x))
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

# ‚úÖ Train model
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = get_model(num_classes=2)
model.to(device)





optimizer = torch.optim.SGD([p for p in model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=0.0005)

for epoch in range(10):
    model.train()
    total_loss = 0
    for images, targets in train_loader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()
        total_loss += losses.item()
    val_acc = evaluate_accuracy(model, valid_loader, device)
    print(f"Epoch {epoch+1}: Loss = {total_loss:.4f}, Accuracy = {val_acc:.2%}")

# ‚úÖ Run inference
predict_and_show(test_image, model, device, threshold=0.3)

# ‚úÖ 1. Install torchmetrics (run this once)
!pip install torchmetrics

# ‚úÖ 2. Add the new import
from torchmetrics.detection import MeanAveragePrecision

# ‚úÖ 3. The new evaluation function (calculates mAP)
def evaluate_map(model, data_loader, device):
    """
    Calculates the mean Average Precision (mAP) for the model.
    """
    model.eval()

    # Instantiate the metric
    metric = MeanAveragePrecision(iou_type="bbox")
    metric.to(device)

    with torch.no_grad():
        for images, targets in data_loader:
            images = [img.to(device) for img in images]

            # The model's outputs are already in the correct format
            outputs = model(images)

            # The targets need to be in the correct format
            metric_targets = []
            for t in targets:
                metric_targets.append({
                    "boxes": t["boxes"].to(device),
                    "labels": t["labels"].to(device)
                })

            # Update the metric with the batch
            metric.update(outputs, metric_targets)

    # Compute the final mAP
    results = metric.compute()
    return results

# ‚úÖ 4. Your updated training loop
print("--- Starting Model Training ---")

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = get_model(num_classes=2)
model.to(device)
optimizer = torch.optim.SGD([p for p in model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=0.0005)

for epoch in range(10):  # 10 epochs
    model.train()
    total_loss = 0

    # --- Training sub-loop ---
    for images, targets in train_loader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        total_loss += losses.item()

    # --- Evaluation sub-loop (called at the end of each epoch) ---
    # This will return a dictionary with 'map', 'map_50', 'map_75', etc.
    map_results = evaluate_map(model, valid_loader, device)
    map_50 = map_results['map_50'] # This is mAP@.50 (the most common one)

    print(f"Epoch {epoch+1}/{10}: Loss = {total_loss:.4f} | Validation mAP@.50: {map_50:.4f}")

print("--- Training Finished ---")


# ‚úÖ 5. Final Evaluation (run this after the loop)
print("\n--- Evaluating Final Model ---")
# Get the final metrics from the fully trained model
final_metrics = evaluate_map(model, valid_loader, device)

print(f"Final mAP: {final_metrics['map']:.4f}")
print(f"Final mAP@.50 (IoU=0.5): {final_metrics['map_50']:.4f}")
print(f"Final mAP@.75 (IoU=0.75): {final_metrics['map_75']:.4f}")

# You can now run inference as before
print("\n--- Running Inference on Test Image ---")
predict_and_show(test_image, model, device, threshold=0.3)

# ‚úÖ 1. Install dependencies
!pip install torch torchvision matplotlib
!pip install torchmetrics

# ‚úÖ 2. Imports
import os
import torch
from PIL import Image
import matplotlib.pyplot as plt
from torchvision import transforms
from torch.utils.data import DataLoader
from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.ops import box_iou
from torchmetrics.detection import MeanAveragePrecision # <-- NEW IMPORT

# ‚úÖ 3. Dataset class (No changes)
class YoloDataset(torch.utils.data.Dataset):
    def __init__(self, images_dir, labels_dir, transforms=None):
        self.images_dir = images_dir
        self.labels_dir = labels_dir
        self.transforms = transforms
        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))])

    def __getitem__(self, idx):
        img_path = os.path.join(self.images_dir, self.image_files[idx])
        label_path = os.path.join(self.labels_dir, self.image_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))

        img = Image.open(img_path).convert("RGB")
        W, H = img.size

        boxes, labels = [], []
        with open(label_path) as f:
            for line in f:
                cls, x_center, y_center, w, h = map(float, line.strip().split())
                x1 = (x_center - w/2) * W
                y1 = (y_center - h/2) * H
                x2 = (x_center + w/2) * W
                y2 = (y_center + h/2) * H
                if x2 > x1 and y2 > y1:
                    boxes.append([x1, y1, x2, y2])
                    labels.append(1)  # Force class to 1 (foreground)

        boxes = torch.as_tensor(boxes if boxes else [[0,0,1,1]], dtype=torch.float32)
        labels = torch.as_tensor(labels if labels else [1], dtype=torch.int64)

        target = {"boxes": boxes, "labels": labels}
        if self.transforms:
            img = self.transforms(img)
        return img, target

    def __len__(self):
        return len(self.image_files)

# ‚úÖ 4. Model setup (No changes)
def get_model(num_classes):
    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
    model = fasterrcnn_resnet50_fpn(weights=weights)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model

# ‚úÖ 5. NEW: Standard Evaluation Function (mAP)
def evaluate_map(model, data_loader, device):
    """
    Calculates the mean Average Precision (mAP) for the model.
    """
    model.eval()

    # Instantiate the metric
    metric = MeanAveragePrecision(iou_type="bbox")
    metric.to(device)

    with torch.no_grad():
        for images, targets in data_loader:
            images = [img.to(device) for img in images]

            # Model outputs are already in the correct format
            outputs = model(images)

            # Format targets for the metric
            metric_targets = []
            for t in targets:
                metric_targets.append({
                    "boxes": t["boxes"].to(device),
                    "labels": t["labels"].to(device)
                })

            # Update the metric
            metric.update(outputs, metric_targets)

    # Compute the final mAP
    results = metric.compute()
    return results

# ‚úÖ 6. Inference and visualization (No changes)
def predict_and_show(image_path, model, device, threshold=0.3):
    print(f"\nüîç Predicting: {image_path}")
    if not os.path.exists(image_path):
        print("‚ùå Image not found.")
        return
    model.eval()
    img = Image.open(image_path).convert("RGB")
    img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = model(img_tensor)
    boxes = outputs[0]['boxes'].cpu().numpy()
    scores = outputs[0]['scores'].cpu().numpy()
    if len(boxes) == 0 or all(score < threshold for score in scores):
        print("‚ö†Ô∏è No objects detected above threshold.")
    else:
        for i, (box, score) in enumerate(zip(boxes, scores)):
            if score >= threshold:
                print(f"Box {i+1}: {box} | Confidence: {score:.2f}")
    plt.imshow(img)
    ax = plt.gca()
    for box, score in zip(boxes, scores):
        if score >= threshold:
            x1, y1, x2, y2 = box
            ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, edgecolor='red', facecolor='none'))
            ax.text(x1, y1, f"{score:.2f}", color='white', backgroundcolor='red')
    plt.axis('off')
    plt.show()

# ‚úÖ 7. Paths and Data (No changes)
base_path = '/content/drive/MyDrive/DL_Datasets/Glass-Breakage-1'
train_images = f'{base_path}/train/images'
train_labels = f'{base_path}/train/labels'
valid_images = f'{base_path}/valid/images'
valid_labels = f'{base_path}/valid/labels'
test_image = f'{base_path}/test/images/DJI_20220409132519_0047_T_JPG.rf.1bda7dca1aacbddcc051c7940324cf18.jpg'

transform = transforms.ToTensor()
train_dataset = YoloDataset(train_images, train_labels, transforms=transform)
valid_dataset = YoloDataset(valid_images, valid_labels, transforms=transform)
collate_fn = lambda x: tuple(zip(*x))
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

# ‚úÖ 8. NEW: Upgraded Training Loop
print("--- Starting Model Training ---")

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = get_model(num_classes=2)
model.to(device)
optimizer = torch.optim.SGD([p for p in model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=0.0005)

num_epochs = 10

# NEW: Lists to store history for plotting
train_losses = []
val_maps = []

# NEW: Variables for model checkpointing
best_map = 0.0
best_epoch = 0

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    # --- Training sub-loop ---
    for images, targets in train_loader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        total_loss += losses.item()

    avg_train_loss = total_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # --- Evaluation sub-loop (using new mAP function) ---
    map_results = evaluate_map(model, valid_loader, device)
    current_map = map_results['map_50'].item() # Get mAP@.50
    val_maps.append(current_map)

    print(f"Epoch {epoch+1}/{num_epochs}: Loss = {avg_train_loss:.4f} | Validation mAP@.50: {current_map:.4f}")

    # --- NEW: Checkpointing ---
    if current_map > best_map:
        best_map = current_map
        best_epoch = epoch + 1
        torch.save(model.state_dict(), 'best_model.pth')
        print(f"   ---> New best model saved at epoch {best_epoch} with mAP: {best_map:.4f}")

print("--- Training Finished ---")
print(f"Best model was from Epoch {best_epoch} with mAP@.50: {best_map:.4f}")


# ‚úÖ 9. NEW: Plotting Graphs
print("\n--- Plotting Training History ---")
plt.figure(figsize=(14, 6))

# Plot 1: Training Loss
plt.subplot(1, 2, 1)
plt.plot(range(1, num_epochs + 1), train_losses, 'b-o')
plt.title('Training Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)

# Plot 2: Validation mAP
plt.subplot(1, 2, 2)
plt.plot(range(1, num_epochs + 1), val_maps, 'r-o')
plt.title('Validation mAP@.50 per Epoch')
plt.xlabel('Epoch')
plt.ylabel('mAP@.50')
# Highlight the best epoch
plt.axvline(x=best_epoch, color='g', linestyle='--', label=f'Best Epoch ({best_epoch})')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()


# ‚úÖ 10. NEW: Run Inference with the BEST Model
print("\n--- Loading best model for inference ---")

# Re-initialize the model structure
model = get_model(num_classes=2)
# Load the saved weights from the best epoch
model.load_state_dict(torch.load('best_model.pth'))
model.to(device)

# Now, run prediction with the *best* model, not the overfitted one
predict_and_show(test_image, model, device, threshold=0.3)

# ‚úÖ This code calculates your final metrics (TP, FP, FN, etc.)

def get_detection_metrics(model, data_loader, device, iou_threshold=0.3):
    """
    Calculates TP, FP, FN, Precision, and Recall for the model.
    """
    model.eval()
    TP, FP, FN = 0, 0, 0

    with torch.no_grad():
        for images, targets in data_loader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            outputs = model(images)

            for pred, target in zip(outputs, targets):
                pred_boxes = pred["boxes"]
                true_boxes = target["boxes"]

                if len(true_boxes) == 0 and len(pred_boxes) == 0:
                    continue # Correctly did nothing
                elif len(true_boxes) == 0:
                    FP += len(pred_boxes) # All predictions are False Positives
                    continue
                elif len(pred_boxes) == 0:
                    FN += len(true_boxes) # All true boxes are False Negatives
                    continue

                # Calculate IoU
                ious = box_iou(pred_boxes, true_boxes)

                # Find best match for each prediction
                # (This is the same logic as your original 'evaluate_accuracy')
                matched_pred = (ious.max(dim=1).values >= iou_threshold)
                # Find best match for each ground truth
                matched_true = (ious.max(dim=0).values >= iou_threshold)

                TP += matched_pred.sum().item()
                FP += (~matched_pred).sum().item()
                FN += (~matched_true).sum().item()

    # Calculate metrics
    # Add 1e-6 to avoid division by zero
    precision = TP / (TP + FP + 1e-6)
    recall = TP / (TP + FN + 1e-6)
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)

    return {
        'TP': TP,
        'FP': FP,
        'FN': FN,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score
    }

# --- How to Run It ---

# 1. Load your best model first (this code is from the previous step)
print("\n--- Loading best model for evaluation ---")
model = get_model(num_classes=2)
model.load_state_dict(torch.load('best_model.pth'))
model.to(device)

# 2. Run the new metrics function on the validation data
print("Calculating final detection metrics...")
final_metrics = get_detection_metrics(model, valid_loader, device)

print("\n--- Final Model Metrics ---")
print(f"True Positives (TP):  {final_metrics['TP']}")
print(f"False Positives (FP): {final_metrics['FP']}")
print(f"False Negatives (FN): {final_metrics['FN']}")
print("---------------------------")
print(f"Precision: {final_metrics['precision']:.2%}")
print(f"Recall:    {final_metrics['recall']:.2%}")
print(f"F1-Score:  {final_metrics['f1_score']:.2%}")

# ‚úÖ This code plots your graphs
# (Run this after the training loop)

print("\n--- Plotting Training History ---")
plt.figure(figsize=(14, 6))

# --- Plot 1: Training Loss ---
plt.subplot(1, 2, 1)
plt.plot(range(1, num_epochs + 1), train_losses, 'b-o')
plt.title('Training Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)

# --- Plot 2: Validation mAP ---
plt.subplot(1, 2, 2)
plt.plot(range(1, num_epochs + 1), val_maps, 'r-o')
plt.title('Validation mAP@.50 per Epoch')
plt.xlabel('Epoch')
plt.ylabel('mAP@.50')
# Highlight the best epoch
plt.axvline(x=best_epoch, color='g', linestyle='--', label=f'Best Epoch ({best_epoch})')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()